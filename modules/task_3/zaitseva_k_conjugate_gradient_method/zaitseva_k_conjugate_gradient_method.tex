\documentclass{report}

\usepackage[warn]{mathtext}
\usepackage[T2A]{fontenc}
\usepackage[utf8]{luainputenc}
\usepackage[english, russian]{babel}
\usepackage[pdftex]{hyperref}
\usepackage{tempora}
\usepackage[12pt]{extsizes}
\usepackage{listings}
\usepackage{color}
\usepackage{geometry}
\usepackage{enumitem}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{indentfirst}
\usepackage{amsmath}

\geometry{a4paper,top=2cm,bottom=2cm,left=2.5cm,right=1.5cm}
\setlength{\parskip}{0.5cm}
\setlist{nolistsep, itemsep=0.3cm,parsep=0pt}

\usepackage{listings}
\lstset{language=C++,
        basicstyle=\footnotesize,
		keywordstyle=\color{blue}\ttfamily,
		stringstyle=\color{red}\ttfamily,
		commentstyle=\color{green}\ttfamily,
		morecomment=[l][\color{red}]{\#}, 
		tabsize=4,
		breaklines=true,
  		breakatwhitespace=true,
  		title=\lstname,       
}

\makeatletter
\renewcommand\@biblabel[1]{#1.\hfil}
\makeatother

\begin{document}

\begin{titlepage}

\begin{center}
Министерство науки и высшего образования Российской Федерации
\end{center}

\begin{center}
Федеральное государственное автономное образовательное учреждение высшего образования \\
Национальный исследовательский Нижегородский государственный университет им. Н.И. Лобачевского
\end{center}

\begin{center}
Институт информационных технологий, математики и механики
\end{center}

\vspace{4em}

\begin{center}
\textbf{\LargeОтчет по лабораторной работе} \\
\end{center}
\begin{center}
\textbf{\Large«Решение систем линейных уравнений методом сопряженных градиентов»} \\
\end{center}

\vspace{4em}

\newbox{\lbox}
\savebox{\lbox}{\hbox{text}}
\newlength{\maxl}
\setlength{\maxl}{\wd\lbox}
\hfill\parbox{7cm}{
\hspace*{5cm}\hspace*{-5cm}\textbf{Выполнила:} \\ студентка группы 381906-3 \\ Зайцева К.А.\\
\\
\hspace*{5cm}\hspace*{-5cm}\textbf{Проверил:}\\ доцент кафедры МОСТ, \\ кандидат технических наук \\ Сысоев А. В.\\
}
\vspace{\fill}

\begin{center} Нижний Новгород \\ 2021 \end{center}

\end{titlepage}

\setcounter{page}{2}

% Содержание
\tableofcontents
\newpage

% Введение
\section*{Введение}
\addcontentsline{toc}{section}{Введение}
\par Метод сопряженных градиентов – один из наиболее известных итерационных методов решения систем линейных уравнений. Он может быть применен для решения системы линейных уравнений с симметричной, положительно определенной матрицей. Матрица $A$ называется положительно определенной, если скалярное произведение $(Ax, x) > 0$ для всех ненулевых векторов. Для того чтобы матрица была положительно определенной необходимо и достаточно, чтобы все ее главные миноры были положительны.
\newpage

% Постановка задачи
\section*{Постановка задачи}
\addcontentsline{toc}{section}{Постановка задачи}
\par В данном лабораторной работе требуется реализовать последовательный и параллельный алгоритмы решения систем линейных уравнений методом сопряженных градиентов, провести вычислительный эксперименты для сравнения времени работы алгоритмов. Тестирование проводилось с помощью Google Test. Параллелизация должна быть реализована через технологию Message Passing Interface (MPI).
\newpage

% Описание алгоритма
\section*{Описание алгоритма}
\addcontentsline{toc}{section}{Описание алгоритма}
\par После выполнения n итераций метода сопряженных градиентов, где n - порядок решаемой СЛУ, очередное приближение $x_n$ совпадает с точным решением. Вычислительная сложность алгоритма $O(n^3)$
\par Если матрица А симметричная и положительно определена, то функция:
$$q(x)=\frac{1}{2}x^TAx - x^Tb + c$$
имеет единственный минимум который достигается в точке $x^*$, совпадающий с решением системы линейных уравнений.
\par Итерация метода сопряженных градиентов состоит в вычислении очередного приближения к точному решению
$$x^k = x^{k-1} + s^kd^k$$
где $x^k$ - очередное приближение, $x^{k-1}$ - приближение на предыдущем шаге, $s^k$ - скалярный шаг, $d^k$ - вектор направления
\par\par \underline{Алгоритм:}
\begin{enumerate} 
	\item Вычисляем градиент: $$g^k = Ax^{k-1} - b$$
	\item Вычисляем вектор направлений: $$d^k = -g^k + \frac{((g^k)^T, g^k)}{((g^{k-1})^T, g^{k-1})^{d^{k-1}}}$$
	\item Вычисление скалярного шага: $$s^k = \frac{(d^k, g^k)}{(d^k)^TAd^k}$$
	\item Вычисление нового приближения
\end{enumerate}
\newpage

% Описание схемы распараллеливания
\section*{Описание схемы распараллеливания}
\addcontentsline{toc}{section}{Описание схемы распараллеливания}
\par Основные вычисления этого метода приходятся на перемножение матрицы А на вектор и также часто используется скалярное произведение векторов. Именно эти подзадачи целесообразно выполнять параллельно. 
\par Рассмотрим параллельный вариант умножения матрицы A на вектор b. Для начала между процессами распределяем строки матрицы A. Допустим у нас есть три процесса и три строки матрицы. Значит каждый процесс получит по одной строке. Также каждый процесс должен иметь свою копию вектора b. После завершения операции каждый процесс будет содержать один из элементов результирующего вектора c. Далее необходимо выполнить операцию сбора данных для объединения результата. 
\par Теперь рассмотрим параллельный алгоритм скалярного произведения. Сначала делим между процессами исходные вектора, вычисляем на них скалярное произведение полученных частей. А затем складываем полученные результаты.
\newpage

% Описание программной реализации
\section*{Описание программной реализации}
\addcontentsline{toc}{section}{Описание программной реализации}
Программа состоит из заголовочного файла conjugateGradientMethod.h и двух файлов исходного кода conjugateGradientMethod.cpp и main.cpp.
\par В заголовочном файле находятся прототипы функций для последовательного и параллельного вычисления систем линейных уравнений методом сопряженных градиентов. В файле conjugateGradientMethod.cpp содержится реализация функций, объявленных в заголовочном файле. Тестирование функционала проводится в файле main.cpp
\par Функция для последовательного алгоритма:
\begin{lstlisting}
std::vector<double> conjugateGradientMethodSerial(double* A, double* b, int n);
\end{lstlisting}
Функция принимает матрицу $A$, вектор $b$ и их размерность n (матрица имеет размерность n*n). Результатом выполнения будет вектор $x$, являющийся решение системы $Ax = b$
\par Функция для параллельного
алгоритма:
\begin{lstlisting}
std::vector<double> conjugateGradientMethodParallel(double* A, double* b,
int n); 
\end{lstlisting}
Параметры и возвращаемое значение данной функции совпадают с функцией для последовательного алгоритма.
\par Рассмотрим, как в этой функции реализовывалось умножение матрицы на вектор. Распределение матрицы $A$ между процессами осуществлялось через функцию MPI\_Scatterv
\begin{lstlisting}
MPI_Scatterv(A, sendcounts.data(), displs.data(), MPI_DOUBLE, aa.data(),
sendcounts[rank], MPI_DOUBLE, 0, COMM_NEW);
\end{lstlisting}
Вектор, который должен быть умножен на матрицу $A$, передавался на все процессы через MPI\_Bcast
\begin{lstlisting}
MPI_Bcast(x_k_1.data(), n, MPI_DOUBLE, 0, COMM_NEW);
\end{lstlisting}
Далее на каждом процессе производятся вычисления части результирующего вектора и сборка данных производится с MPI\_Gatherv
\begin{lstlisting}
MPI_Gatherv(res.data(), sendcounts[rank] / n, MPI_DOUBLE, g_k.data(),
recvcounts.data(), displs.data(), MPI_DOUBLE, 0, COMM_NEW);
\end{lstlisting}
В результате выполнения этого кода мы получим результат умножения матрицы $A$ на вектор $x\_k\_1$, записанный в вектор $g\_k$.
\par Теперь разберем реализацию скалярного произведения. Для обоих векторов вызывалась функции MPI\_Bcast, далее вычислялись частичные скалярные произведения и результат собирался через функцию MPI\_Reduce
\begin{lstlisting}
MPI_Reduce(&sum, &sum_all, 1, MPI_DOUBLE, MPI_SUM, 0, COMM_NEW);
\end{lstlisting}
\newpage

% Подтверждение корректности
\section*{Подтверждение корректности}
\addcontentsline{toc}{section}{Подтверждение корректности}
Для определения корректности разработанных функции были подготовлены 5 тестов. В каждом из них решение системы линейных уравнений находится сначала с помощью последовательного метода, а потом с помощью параллельного. Результаты сравниваются в пределах некоторой погрешности. Успешное прохождение тестов подтверждает корректность алгоритмов.
\newpage

% Результаты экспериментов
\section*{Результаты экспериментов}
\addcontentsline{toc}{section}{Результаты экспериментов}
Вычислительные эксперименты для оценки эффективности работы параллельного алгоритма проводились на ПК со следующими характеристиками:
\begin{itemize}
\item Процессор: Intel Core i7-8565U, 1.8 ГГц, количество ядер: 4
\item Оперативная память: 16 ГБ (DDR4), 2400 МГц
\item Операционная система: Windows 10 Home
\end{itemize}

\par Эксперименты проводились на одном, двух и четырех процесса. На разных размерностях матриц.

\par Результаты экспериментов представлены в таблице 1. Время указано в секундах. 

\begin{table}[!h]
\caption{Результаты вычислительных экспериментов}
\centering
\begin{tabular}{| p{3cm} | p{4cm} | p{4cm} | p{4cm} |}
\hline
Размерность матрицы & Последовательный\par алгоритм & Параллельный\par алгоритм в 2 процесса & Параллельный \par алгоритм в 4 процесса  \\[5pt]
\hline
100x100    & 0.0599743   & 0.0454734   & 0.0369003       \\
\hline
500x500    & 7.83521     & 5.97487     & 3.94156  \\
\hline
1000x1000  & 60.7891     & 47.6059     & 32.6048       \\
\hline
\end{tabular}
\end{table}

\newpage

% Выводы из результатов экспериментов
\section*{Выводы из результатов экспериментов}
\addcontentsline{toc}{section}{Выводы из результатов экспериментов}
Из данных, полученных в результате экспериментов (см. Таблицу 1), можно сделать вывод, что параллельный алгоритм работает значительно быстрее последовательного. Также ускорение возрастает при увеличении числа процессов. Используя 4 процесса удается достичь почти двукратного ускорения. Так как физических ядра всего четыре проводить эксперименты с большим количеством процессов нецелесообразно. 

\newpage

% Заключение
\section*{Заключение}
\addcontentsline{toc}{section}{Заключение}
Таким образом, в рамках данной лабораторной работы были разработаны последовательный и параллельный алгоритмы решение систем линейных уравнений метод сопряженных градиентов. Корректность реализованной программы подтверждена с помощью тестирования, а проведенные эксперименты доказали эффективность параллельного алгоритма по сравнению с последовательным.
\newpage

% Литература
\section*{Литература}
\addcontentsline{toc}{section}{Литература}
\begin{enumerate}
\item Самарский А. А., Гулин А. В. Численные методы.— М.: Наука, 1989
\item Васильев Ф. П.   Методы оптимизации - Издательство «Факториал Пресс», 2002
\item Habr - Электронный ресурс. URL: https://habr.com/ru/post/413853/
\item Википедия - Электронный ресурс. URL: https://ru.wikipedia.org/wiki/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4_%D1%81%D0%BE%D0%BF%D1%80%D1%8F%D0%B6%D1%91%D0%BD%D0%BD%D1%8B%D1%85_%D0%B3%D1%80%D0%B0%D0%B4%D0%B8%D0%B5%D0%BD%D1%82%D0%BE%D0%B2_(%D0%B4%D0%BB%D1%8F_%D1%80%D0%B5%D1%88%D0%B5%D0%BD%D0%B8%D1%8F_%D0%A1%D0%9B%D0%90%D0%A3)
\end{enumerate} 
\newpage

% Приложение
\section*{Приложение}
\addcontentsline{toc}{section}{Приложение}

conjugateGradientMethod.h
\begin{lstlisting}
// Copyright 2021 Zaitseva Ksenia
#ifndef MODULES_TASK_3_ZAITSEVA_K_CONJUGATE_GRADIENT_METHOD_CONJUGATEGRADIENTMETHOD_H_
#define MODULES_TASK_3_ZAITSEVA_K_CONJUGATE_GRADIENT_METHOD_CONJUGATEGRADIENTMETHOD_H_
#include <mpi.h>

#include <iostream>
#include <vector>
#include <random>

double ScalarMult(std::vector<double> x, std::vector<double> y);
std::vector<double> conjugateGradientMethodSerial(double* A, double* b, int n);
std::vector<double> conjugateGradientMethodParallel(double* A, double* b,
int n);

#endif  // MODULES_TASK_3_ZAITSEVA_K_CONJUGATE_GRADIENT_METHOD_CONJUGATEGRADIENTMETHOD_H_

\end{lstlisting}
conjugateGradientMethod.cpp
\begin{lstlisting}
// Copyright 2021 Zaitseva Ksenia
#include "conjugateGradientMethod.h"

double ScalarMult(std::vector<double> x, std::vector<double> y) {
	double res = 0;
	int n = x.size();
	for (int i = 0; i < n; i++) res += x[i] * y[i];
	return res;
}
double ScalarMultParallel(double* x, double* y, int n, MPI_Comm COMM_NEW) {
	int size, rank;
	MPI_Comm_size(COMM_NEW, &size);
	MPI_Comm_rank(COMM_NEW, &rank);
	
	double sum = 0, sum_all = 0;
	for (int i = rank; i < n; i += size) sum += x[i] * y[i];
	MPI_Reduce(&sum, &sum_all, 1, MPI_DOUBLE, MPI_SUM, 0, COMM_NEW);
	return sum_all;
}

std::vector<double> conjugateGradientMethodSerial(double* A, double* b, int n) {
	std::vector<double> x_k_1(n, 0);
	std::vector<double> x_k(n, 0);
	std::vector<double> g_k(n, 0);
	std::vector<double> delta(n);
	
	for (int i = 0; i < n; i++) {
		x_k_1 = x_k;
	
	for (int j = 0; j < n; j++) {
		g_k[j] = 0;
		for (int k = 0; k < n; k++) g_k[j] += A[j * n + k] * x_k_1[k];
		g_k[j] -= b[j];
	}
	
	std::vector<double> r1(n);
	for (int j = 0; j < n; j++) {
		for (int k = 0; k < n; k++) r1[j] += A[j * n + k] * g_k[k];
	}
	
	if (i > 0) {
		std::vector<double> r2(n);
		for (int j = 0; j < n; j++) {
			for (int k = 0; k < n; k++) r2[j] += A[j * n + k] * delta[k];
		}
		double r3 = ScalarMult(g_k, g_k);
		double r4 = ScalarMult(g_k, delta);
		double r5 = ScalarMult(r1, delta);
		double r6 = ScalarMult(r1, g_k);
		double r7 = ScalarMult(r2, delta);
		double alpha = (r3 * r5 - r4 * r6) / (r7 * r6 - r5 * r5);
		double beta = (r4 * r5 - r3 * r7) / (r7 * r6 - r5 * r5);
		for (int j = 0; j < n; j++) delta[j] = alpha * delta[j] + beta * g_k[j];
		} else {
			double r2 = ScalarMult(g_k, g_k);
			double r3 = ScalarMult(r1, g_k);
			double B_k = -r2 / r3;
			for (int j = 0; j < n; j++) delta[j] = B_k * g_k[j];
		}
		for (int j = 0; j < n; j++) x_k[j] = x_k_1[j] + delta[j];
	}
	return x_k;
}

std::vector<double> conjugateGradientMethodParallel(double* A, double* b,
int n) {
	int size, rank;
	MPI_Comm_size(MPI_COMM_WORLD, &size);
	MPI_Comm_rank(MPI_COMM_WORLD, &rank);
	int s = size;
	if (size > n) s = n;
	std::vector<int> ranks(s);
	for (int i = 0; i < s; i++) ranks[i] = i;
	MPI_Group group;
	MPI_Comm_group(MPI_COMM_WORLD, &group);
	MPI_Group new_group;
	MPI_Group_incl(group, s, ranks.data(), &new_group);
	MPI_Comm COMM_NEW;
	MPI_Comm_create(MPI_COMM_WORLD, new_group, &COMM_NEW);
	std::vector<double> x_k(n);
	if (rank == 0) {
		for (int i = 0; i < n; i++) x_k[i] = 0.0;
	}
	if (COMM_NEW != MPI_COMM_NULL) {
		MPI_Comm_size(COMM_NEW, &size);
		MPI_Comm_rank(COMM_NEW, &rank);
		std::vector<int> displs(size);
		std::vector<int> sendcounts(size);
		for (int i = 0; i < size; i++) {
			displs[i] =
			static_cast<int>((static_cast<int>(n * n / size) * i) / n) * n;
		}
		for (int i = 0; i < size - 1; i++) {
			sendcounts[i] = displs[i + 1] - displs[i];
		}
		sendcounts[size - 1] = n * n - displs[size - 1];
		std::vector<double> aa(sendcounts[rank]);
		
		MPI_Scatterv(A, sendcounts.data(), displs.data(), MPI_DOUBLE, aa.data(),
		sendcounts[rank], MPI_DOUBLE, 0, COMM_NEW);
		std::vector<double> delta(n);
		std::vector<int> recvcounts(size);
		std::vector<double> x_k_1(n);
		std::vector<double> res(sendcounts[rank] / n);
		std::vector<double> g_k(n);
		std::vector<double> r1(n);
		std::vector<double> r2(n);
		for (int p = 0; p < n;) {
		if (rank == 0) {
			for (int i = 0; i < n; i++) x_k_1[i] = x_k[i];
		}
		MPI_Bcast(x_k_1.data(), n, MPI_DOUBLE, 0, COMM_NEW);
		for (int i = 0; i < (sendcounts[rank] / n); i++) {
			res[i] = 0.0;
			for (int k = 0; k < n; k++) res[i] += aa[i * n + k] * x_k_1[k];
		}
		for (int i = 0; i < size; i++) {
			recvcounts[i] = sendcounts[i] / n;
		}
		displs[size - 1] = n - recvcounts[size - 1];
		for (int i = size-2; i >= 0; i--) {
		displs[i] = displs[i + 1] - recvcounts[i];
		}
		MPI_Gatherv(res.data(), sendcounts[rank] / n, MPI_DOUBLE, g_k.data(),
		recvcounts.data(), displs.data(), MPI_DOUBLE, 0, COMM_NEW);
		
		if (rank == 0) {
			for (int j = 0; j < n; j++) g_k[j] -= b[j];
		}
		
		MPI_Bcast(g_k.data(), n, MPI_DOUBLE, 0, COMM_NEW);
		for (int i = 0; i < (sendcounts[rank] / n); i++) {
			res[i] = 0.0;
			for (int k = 0; k < n; k++) res[i] += aa[i * n + k] * g_k[k];
		}
		for (int i = 0; i < size; i++) {
			recvcounts[i] = sendcounts[i] / n;
		}
		displs[size - 1] = n - recvcounts[size - 1];
		for (int i = size - 2; i >= 0; i--) {
			displs[i] = displs[i + 1] - recvcounts[i];
		}
		MPI_Gatherv(res.data(), sendcounts[rank] / n, MPI_DOUBLE, r1.data(),
		recvcounts.data(), displs.data(), MPI_DOUBLE, 0, COMM_NEW);
		
		if (p > 0) {
			MPI_Bcast(delta.data(), n, MPI_DOUBLE, 0, COMM_NEW);
			for (int i = 0; i < (sendcounts[rank] / n); i++) {
				res[i] = 0.0;
				for (int k = 0; k < n; k++) res[i] += aa[i * n + k] * delta[k];
			}
			for (int i = 0; i < size; i++) {
				recvcounts[i] = sendcounts[i] / n;
			}
			displs[size - 1] = n - recvcounts[size - 1];
			for (int i = size - 2; i >= 0; i--) {
				displs[i] = displs[i + 1] - recvcounts[i];
			}
			MPI_Gatherv(res.data(), sendcounts[rank] / n, MPI_DOUBLE, r2.data(),
			recvcounts.data(), displs.data(), MPI_DOUBLE, 0, COMM_NEW);
			
			MPI_Bcast(g_k.data(), n, MPI_DOUBLE, 0, COMM_NEW);
			double r3 = ScalarMultParallel(g_k.data(), g_k.data(), n, COMM_NEW);
			double r4 = ScalarMultParallel(g_k.data(), delta.data(), n, COMM_NEW);
			MPI_Bcast(r1.data(), n, MPI_DOUBLE, 0, COMM_NEW);
			double r5 = ScalarMultParallel(r1.data(), delta.data(), n, COMM_NEW);
			double r6 = ScalarMultParallel(r1.data(), g_k.data(), n, COMM_NEW);
			MPI_Bcast(r2.data(), n, MPI_DOUBLE, 0, COMM_NEW);
			double r7 = ScalarMultParallel(r2.data(), delta.data(), n, COMM_NEW);
			if (rank == 0) {
				double alpha = (r3 * r5 - r4 * r6) / (r7 * r6 - r5 * r5);
				double beta = (r4 * r5 - r3 * r7) / (r7 * r6 - r5 * r5);
				for (int j = 0; j < n; j++)
				delta[j] = alpha * delta[j] + beta * g_k[j];
			}
			} else {
				MPI_Bcast(g_k.data(), n, MPI_DOUBLE, 0, COMM_NEW);
				double r8 = ScalarMultParallel(g_k.data(), g_k.data(), n, COMM_NEW);
				MPI_Bcast(r1.data(), n, MPI_DOUBLE, 0, COMM_NEW);
				double r9 = ScalarMultParallel(r1.data(), g_k.data(), n, COMM_NEW);
				if (rank == 0) {
					double B_k = -r8 / r9;
					for (int j = 0; j < n; j++) delta[j] = B_k * g_k[j];
				}
			}
			if (rank == 0) {
				for (int j = 0; j < n; j++) x_k[j] = x_k_1[j] + delta[j];
				p++;
			}
			MPI_Bcast(&p, 1, MPI_INT, 0, COMM_NEW);
		}
		MPI_Comm_free(&COMM_NEW);
	}
	MPI_Bcast(x_k.data(), 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);
	return x_k;
}
\end{lstlisting}
main.cpp
\begin{lstlisting}
// Copyright 2021 Zaitseva Ksenia
#include <gtest/gtest.h>
#include <mpi.h>

#include <cmath>
#include <gtest-mpi-listener.hpp>

#include "conjugateGradientMethod.h"

TEST(Parallel_MPI, Test_n3) {
	int rank;
	MPI_Comm_rank(MPI_COMM_WORLD, &rank);
	double A[9] = {4, -1, 2, -1, 6, -2, 2, -2, 5};
	double b[3] = {-1, 9, -10};
	int n = 3;
	std::vector<double> res_seq;
	std::vector<double> res_par = conjugateGradientMethodParallel(A, b, n);
	if (rank == 0) {
		res_seq = conjugateGradientMethodSerial(A, b, n);
		for (int i = 0; i < n; i++)
			EXPECT_NEAR(res_seq[i], res_par[i],
				std::numeric_limits<double>::epsilon() * 16 * 1000);
	}
}

TEST(Parallel_MPI, Test_n4) {
	int rank;
	MPI_Comm_rank(MPI_COMM_WORLD, &rank);
	double A[16] = {1, 0, 3, 2, 0, 1, 0, 0, 3, 0, 1, 0, 2, 0, 0, 1};
	double b[4] = {0, -3, 0.5, 0};
	int n = 4;
	std::vector<double> res_par = conjugateGradientMethodParallel(A, b, n);
	if (rank == 0) {
		std::vector<double> res_seq = conjugateGradientMethodSerial(A, b, n);
		for (int i = 0; i < n; i++) {
			EXPECT_NEAR(res_seq[i], res_par[i],
				std::numeric_limits<double>::epsilon() * 16 * 1000);
		}
	}
}

TEST(Parallel_MPI, Test_n4_2) {
int rank;
MPI_Comm_rank(MPI_COMM_WORLD, &rank);
double A[16] = {1, 2, 3, 4, 2, 2, 3, 7, 3, 3, 3, 0, 4, 7, 0, 4};
double b[4] = {1, 3, -1, 7};
int n = 4;
std::vector<double> res_par = conjugateGradientMethodParallel(A, b, n);
if (rank == 0) {
	std::vector<double> res_seq = conjugateGradientMethodSerial(A, b, n);
	for (int i = 0; i < n; i++) {
		EXPECT_NEAR(res_seq[i], res_par[i],
			std::numeric_limits<double>::epsilon() * 16 * 1000);
	}
}
}

TEST(Parallel_MPI, Test_n2_2) {
int rank;
MPI_Comm_rank(MPI_COMM_WORLD, &rank);
double A[4] = {-11, 6, 6, -11};
double b[2] = {1, 0};
int n = 2;
std::vector<double> res_seq;
std::vector<double> res_par = conjugateGradientMethodParallel(A, b, n);
if (rank == 0) {
	res_seq = conjugateGradientMethodSerial(A, b, n);
	for (int i = 0; i < n; i++)
		EXPECT_NEAR(res_seq[i], res_par[i],
			std::numeric_limits<double>::epsilon() * 16 * 1000);
	}
}

TEST(Parallel_MPI, Test_n5) {
	int rank;
	MPI_Comm_rank(MPI_COMM_WORLD, &rank);
	int n = 5;
	double A[25] = {1, 1, 1, 1, 1, 1, 2, 3, 4, 5, 1, 3, 6, 10, 15, 1, 4, 10, 20, 35, 1, 5, 15, 35, 70};
	double b[5] = {1, 3, -1, 7};
	std::vector<double> res_seq;
	std::vector<double> res_par =
		conjugateGradientMethodParallel(A, b, n);
	if (rank == 0) {
		res_seq = conjugateGradientMethodSerial(A, b, n);
		for (int i = 0; i < n; i++)
			EXPECT_NEAR(res_seq[i], res_par[i],
				std::numeric_limits<double>::epsilon() * 16 * 100000000);
		}
	}
}

int main(int argc, char** argv) {
	::testing::InitGoogleTest(&argc, argv);
	MPI_Init(&argc, &argv);
	::testing::AddGlobalTestEnvironment(new GTestMPIListener::MPIEnvironment);
	::testing::TestEventListeners& listeners =
	::testing::UnitTest::GetInstance()->listeners();
	listeners.Release(listeners.default_result_printer());
	listeners.Release(listeners.default_xml_generator());
	listeners.Append(new GTestMPIListener::MPIMinimalistPrinter);
	return RUN_ALL_TESTS();
}
\end{lstlisting}

\end{document}