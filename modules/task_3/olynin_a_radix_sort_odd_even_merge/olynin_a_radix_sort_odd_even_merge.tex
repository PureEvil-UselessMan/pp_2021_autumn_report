\documentclass[12pt]{report}
\usepackage{lingmacros}
\usepackage{tree-dvips}
\usepackage[utf8]{luainputenc}
\usepackage[english, russian]{babel}
\usepackage[T2A]{fontenc}
\usepackage{geometry}
\usepackage{listings}
\usepackage[pdftex]{hyperref}
\usepackage[14pt]{extsizes}
\usepackage{color}
\usepackage{enumitem}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{indentfirst}
\usepackage{amsmath}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows.meta,arrows}

\geometry{a4paper, top=2cm, bottom=3cm, left=2.5cm, right=1.5cm}

\setlength{\parskip}{0.5cm}
\setlist{nolistsep, itemsep=0.3cm, parsep=0pt}
\lstset{language=C++,
        basicstyle=\footnotesize,
        keywordstyle=\color{blue}\ttfamily,
        stringstyle=\color{red}\ttfamily,
        commentstyle=\color{green}\ttfamily,
        morecomment=[l][\color{magenta}]{\#},
        tabsize=4,
        breaklines=true,
        breakatwhitespace=true,
        title=\lstname}

\begin{document}
\begin{titlepage}
\begin{center}
    Министерство образования и науки Российской Федерации \\
    Федеральное государственное автономное образовательное учреждение высшего образования
\end{center}
\begin{center}
    \textbf{Национальный исследовательский Нижегородский государственный университет им. Н.И. Лобачевского} \\
\end{center}
\begin{center}
    \textbf{Институт информационных технологий, математики и механики}\\
\end{center}

\vspace{4em}

\begin{center}
    \textbf{\Large Отчет по лабораторной работе} \\
\end{center}

\begin{center}
    \textbf{Тема:} \\
    \textbf{\Large «Поразрядная сортировка для целых чисел с четно-нечетным слиянием Бэтчера.»}
\end{center}

\vspace{4em}

\begin{flushright}
\begin{minipage}{0.55\textwidth}
\begin{flushleft}

\textbf{Выполнил:} \\
студент группы 381906-3 \\
Олюнин А.В. \\

\textbf{Проверил:} \\
доцент кафедры МОСТ, \\
кандидат технических наук \\
Сысоев А.В. \\
\end{flushleft}
\end{minipage}
\end{flushright}


\vspace{\fill}

\begin{center}
Нижний Новгород \\
2021
\end{center}

\end{titlepage}

\setcounter{page}{2}

\tableofcontents

\newpage

\section*{Введение}
\addcontentsline{toc}{section}{Введение}
\parВ данной лабораторной работе затрагивается распространненая проблема сортировки некоторого объема данных. Причем важно отметить отличительную особенность рассматриваемых данных - числа целые. Это свойство дает просторное поле для создания наиболее эффективного алгоритма сортировки таких данных. Одним из таких по праву является алгоритм порязрядной сортировки.
\parНужно понимать, что рассматриваемые массивы данных достаточно независимости друг от друга, благодаря чему предоставляется возможность их параллельного анализиза и согласованной работы с ними. Закономерно этому принципу вполне вероятно существуют методы распараллеливания программы, выполняющей поразрядную сортировку данных, о чем далее и пойдет речь в этой лабораторной работе.

\newpage


\section*{Постановка задачи}
\addcontentsline{toc}{section}{Постановка задачи}
\parЦель данной лабораторной состоит в решении задачи сортировки целых чисел, путем изучения алгоритма поразрядной сортировки и написания программного кода, реализующего последовательную и параллельную обработку массива данных с его помощью. Кроме того, необходимо проверить решение на верность получаемого результата, провести сравнение повышения производительности вычислений относительно скорости работы программы между последовательным и параллельным алгоритмом. Для достижения поставленной задачи необходимо использовать программное обеспечение на основе технологии MPI и платформу тестирования программ Google Test.

\newpage


\section*{Описание алгоритма}
\addcontentsline{toc}{section}{Описание алгоритма}
\parСуть алгоритма заключается в эффективном использовании характеристики сортируемых данных, а именно так как они целочисленные, то они обладают такой особенностью, как наличие разрядности. Что и используется в алгоритме поразрядной сортировки, достигая его отсортированности, выполняя ряд простых, но хитрых шагов. Во-первых, необходимо определить максимальную степень десятки на которой сортировку следует закончить. Программными средствами этого можно достичь находя максимум за один проход, после чего определить степень десятки, при которой остаток и есть само число. Во-вторых, требуется разбить исходный неотсортированный массив ровно на десять подмассивов, каждый из которых соответствует одной уникальной арабской цифре. В-третьих, преобразовать полученную структуру в новый массив, который может быть неотсортированным или отсортированным в зависимости от шага итерации.
\parТаким образом, вытекает ряд характеристик, присущим данной сортировке, а именно:
\begin{itemize}
    \item Линейная временная сложность $O(n)$
    \item Устойчивость
    \item Отсутствие операций сравнения
\end{itemize}

\newpage


\section*{Описание схемы распараллеливания}
\addcontentsline{toc}{section}{Описание схемы распараллеливания}
\parК сожалению, зачастую случается так, что лучший последовательный алгоритм имеет сугубо последовательную природу, поэтому схема распараллеливания данного алгоритма представляет собой разбиение массива на блоки с целью их сортировки и дальнейшего слияния, что можно представить в виде нескольких этапов работы с массивами данных.
\parНа этапе подготовки, некоторый данный в условии неупорядоченный массив целых чисел разбивается на потенциально равные части, которые раздаются процессам участвующим в операции. В случае наличия остатка при таком делении, он распределяется между процессами коммуникатора.
\parНа следующем этапе каждый процесс выполняет поразрядную сортировку полученного им подмассива. Таким образом получается, что каждый процесс обладает своим отсортированным массивом, и теперь стоит задача слить все массивы в один. Тут есть два основных пути: воспользоваться обычным слиянием или слиянием Бэтчера. Отличие заключается в том, что обычное слияние возможно выполнять одновременно только на одном процессе, в то время как другой будет простаивать. Для того, чтобы обойти эту проблему можно воспользоваться методом ЧетНечет слияния. Его суть состоит в разбиении каждого сливаемого массива на четные и нечетные элементы, а затем каждый процесс выполнит слияние четных и нечетных частей двух массивов (схема 4.1). На последней итерации слияние Бэтчера будут выполнять два процесса.

\tikzstyle{decision} = [diamond, draw, text width=6em, text centered,
node distance=3cm, inner sep=0pt]
\tikzstyle{block} = [rectangle, draw, text width=6em, text centered,
minimum height=2em]
\tikzstyle{line} = [draw, -latex']

\begin{tikzpicture}[node distance = 2cm, auto]
    \node [block] (l1b1) {$N / 4$};
    \node [block, right of = l1b1, node distance=4cm](l1b2){$N / 4$};
    \node [block, right of = l1b2, node distance=4cm](l1b3){$N / 4$};
    \node [block, right of = l1b3, node distance=4cm](l1b4){$N / 4$};
    \node [block, below of = l1b1, right of = l1b1](l2b1){$N / 2$};
    \node [block, below of = l1b3, right of = l1b3](l2b2){$N / 2$};
    \path [line](l1b1) |- node [near start] {$OddEvenMerge$} (l2b1);
    \path [line](l1b2) |- (l2b1);
    \path [line](l1b3) |- node [near start] {$OddEvenMerge$} (l2b2);
    \path [line](l1b4) |- (l2b2);
    \node [block, below of = l2b1, right of = l2b1, node distance = 4cm] (l3b1){$N$};
    \path [line](l2b1) |- node [near start] {$OddEvenMerge$} (l3b1);
    \path [line](l2b2) |- (l3b1);
\end{tikzpicture}

\parВ результате такого слияния получится массив, требующий дополнительного прохода, с целью сравнить элементы на четных и нечетных позициях и заменить их местами в случае неупорядоченности. Причем нужно избежать сравнения первого и последнего элементов массива, так как они являются глобальными минимумом и максимумом соответственно.
\parТаким образом, достигнута высокая эффективность распараллеленного метода поразряднойсортировки.

\newpage


\section*{Описание программной реализации}
\addcontentsline{toc}{section}{Описание программной реализации}
\parПрограммная реализация алгоритма поразрядной сортировки представляет собой набор следующих функций:
\par 1. Функция поразрядной сортировки
\begin{lstlisting}
std::vector<int> RadixSort(std::vector<int> main_data, int n);
\end{lstlisting}
\parОписание: реализует полную поразрядную сортировку массива целочисленных данных на одном из процессов.

\par 2. Функция извлечения максимльной степени
\begin{lstlisting}
    int GetMaxPower(std::vector<int> data);
\end{lstlisting}
\parОписание: реализует вычисление максимальной степени, на которой необходимо закончить поразрядную сортировку.

\par 3. Функция обычного слияния
\begin{lstlisting}
std::vector<int> Merge(std::vector<int> first, std::vector<int> second);
\end{lstlisting}
\parОписание: реализует обычный алгоритм слияния двух упорядоченных массивов, может выполняться только на одном процессе.

\par 4. Функция четнечет слияния Бэтчера
\begin{lstlisting}
std::vector<int> OddEvenMerge(std::vector<int> first, std::vector<int> second);
\end{lstlisting}
\parОписание: реализует алгоритм слияния четных или нечетных элементов двух массивов.

\par 5. Разделитель на четные
\begin{lstlisting}
void EvenSplitter(std::vector<int>* first, std::vector<int>* second);
\end{lstlisting}
\parОписание: реализует извлечение всех четных элементов массивов.

\par 6. Разделитель на нечетные
\begin{lstlisting}
void OddSplitter(std::vector<int>* first, std::vector<int>* second);
\end{lstlisting}
\parОписание: реализует извлечение всех нечетных элементов массивов.

\par 7. Функция завершающей операции слияния
\begin{lstlisting}
std::vector<int> GetResult(std::vector<int> first, std::vector<int> second);
\end{lstlisting}
\parОписание: реализует формирование слитого массива, сравнение и перестановку элементов на четных и нечетных позициях.

\par 8. Главная функция параллельной поразрядной сортировки со слиянием Бэтчера
\begin{lstlisting}
std::vector<int> ParallelRadixSortWithOddEvenMerge(std::vector<int> data_root);
\end{lstlisting}
\parОписание: реализует параллельный алгоритм поразрядной сортировки с использованием четнечет слиянием, обеспечивает логику общения процессов при работе с массивами.

\par 9. Функция выполнения межпроцессорного общения для процесса четного ранга
\begin{lstlisting}
std::vector<int> EvenCountProcess(std::vector<int> data_local, int partner);
\end{lstlisting}
\parОписание: реализует обмен сообщениями между двумя процессами с целью слияния четных элементов двух массивов, выполняется для процесса четного ранга.

\par 10. Функция выполнения межпроцессорного общения для процесса нечетного ранга
\begin{lstlisting}
void OddCountProcess(std::vector<int> second, int partner);
\end{lstlisting}
\parОписание: реализует обмен сообщениями между двумя процессами с целью слияния нечетных элементов двух массивов, выполняется для процесса нечетного ранга.

\par 11. Функция создания случайного массива целочисленных данных
\begin{lstlisting}
std::vector<int> GetRandomData(int amount, int dist);
\end{lstlisting}
\parОписание: реализует создание псевдослучайного массива данных в виде неотсортированного вектора целых чисел.

\newpage

\section*{Тестирование параллельной программы}
\addcontentsline{toc}{section}{Тестирование параллельной программы}
Для того, чтобы убедиться в корректности работы программы, производились небольшие тесты, в первом результат поразрядной сортировки проверялся с помощью сортировки этого же массива при помощи встроенной функции сортировки языка C++, в другом проверялась работа алгоритма слияния на подготовленных заранее массивах. Данные тесты были проведены при помощи Google Tests, ошибок не выялено.
\parПосле чего, произведилась оценка эффективности параллельного решения относительно времени по сравнению с последовательным. В качестве исходных данных был взят псевдослучайно сгенерированный массив целочисленных данных размера $10^{6}$ элементов, результаты полученные в посредством эксперимета представлены в таблице 1:

\begin{table}[!h]
    \centering
    \begin{tabular}{ | l | l | l | l |}
    \hline
    \scriptsize{Кол-во процессов} & \scriptsize{Последовательный алгоритм (сек.)} & \scriptsize{Параллельный алгоритм (сек.)} & \scriptsize{Ускорение}  \\ \hline
    2   &   0.362774    &   0.192784    &   1.881763 \\ \hline
    3   &   0.362826    &   0.150214    &   2.415399  \\ \hline
    4   &   0.353755    &   0.141912    &   2.492778  \\ \hline
    7   &   0.353144    &   0.159777    &   2.210235  \\ \hline
    8   &   0.347856    &   0.154115    &   2.257121  \\ \hline
    17   &  0.347477    &   0.173308    &   2.004971  \\ \hline
    \end{tabular}
    \caption{Результаты теста на массиве $10^6$}
    \end{table}

\parПроанализировав данную таблицу становится очевиден тот факт, что наилучшее время показывают тесты, проводившиеся на немногим более чем двух процессах. При этом 2 процесса показывают наилучшую эффективность с точки зрения повышения скорости вычислений при меньших затратах ресурсов. Однако, при использовании несколько больше процессов время выполнения программы снова начинает расти. Подобные наблюдения оправдываются паралльной реализацией алгоритма. Так как сам алгоритм имеет линейную сложность, то при распределении некоторого массива на процессы потенциал ускорения тоже будет линейным, но это утверждение может быть не совсем верно при размере массива, сильно превышающем объем первичной кэш памяти, что приведет к латентной подгрузки элементов массива из памяти более высоких уровней. Кроме того, на производительность вычислений может сказаться нелинейность памяти.
\parТем не менее, то свойство, что алгоритм не содержит в самом себе параллельности предполагает тенденцию на неэффективное использование достаточно большого числа процессов, ведь слияние большого числа отсортированных кусочков массива оказывает существенное негативное влияние на время работы параллельного алгоритма.

\newpage


\section*{Заключение}
\addcontentsline{toc}{section}{Заключение}
\parВ данной лабораторной работе был рассмотрен алгоритм поразрядной сортировки целых чисел, не только его последовательная реализация, но и методы его распараллеливания. Была написана параллельная программа, выполненная при помощи технологии MPI, реализующая данную сортировку. Однако, по причине того, что она имеет последовательную природу, стандартным решением в таком случае стало разбиение массива на несколько частей с целью их сортировки и дальнейшего слияния. Операция слияния двух отсортированных массивов была выполнена в виде четнечет слияния Бэтчера, которую возможно одновременно выполнять на двух процессах.
\parВ ходе работы был произведен ряд тестов, нацеленных на подтверждение безошибочности работы программы и рациональности применения параллеьного метода поразрядной сортировки. Результаты показали целесообразность использования нескольких процессов для данной программной реализации алгоритма.

\newpage


\begin{thebibliography}{1}
\addcontentsline{toc}{section}{Список литературы}
\bibitem{Sisoev}Сысоев А. В., Мееров И. Б., Свистунов А. Н., Курылев А. Л., Сенин А. В., Шишков А. В., Корняков К. В.,
Сиднев А. А. Параллельное программирование в системах с общей
памятью. Инструментальная поддержка, Нижний Новгород, 2007.
\bibitem{Sidnev}Сиднев А.А., Сысоев А.В., Мееров И.Б. Оптимизация вычислительно трудоемкого программного модуля для архитектуры Intel Xeon Phi. Линейные сортировки, Нижний Новгород, 2013.
\bibitem{Yakobovski}Якобовский М.В. Параллельные алгоритмы сортировки больших объемов данных, 2004, 2008.
\bibitem{Knyt}Кнут Д. Искусство программирования, т.3. Сортировка и поиск. 2-е издание. – М.: Вильямс, 2009 . – 824 с.
\end{thebibliography}
\newpage

\section*{Приложение}
\addcontentsline{toc}{section}{Приложение}
В разделе приводится код, написаный в рамках данной лабораторной работы.


\begin{lstlisting}
// radix_sort_odd_even_merge.h
// Copyright 2021 Olynin Alexandr
#include <vector>

#ifndef MODULES_TASK_3_OLYNIN_A_RADIX_SORT_ODD_EVEN_MERGE_RADIX_SORT_ODD_EVEN_MERGE_H_
#define MODULES_TASK_3_OLYNIN_A_RADIX_SORT_ODD_EVEN_MERGE_RADIX_SORT_ODD_EVEN_MERGE_H_

std::vector<int> RadixSort(std::vector<int> arr, int n);
int GetMaxPower(std::vector<int> data);
std::vector<int> Merge(std::vector<int> first, std::vector<int> second);
std::vector<int> OddEvenMerge(std::vector<int> first, std::vector<int> second);

void EvenSplitter(std::vector<int>* first, std::vector<int>* second);
void OddSplitter(std::vector<int>* first, std::vector<int>* second);
std::vector<int> GetResult(std::vector<int> first, std::vector<int> second);

std::vector<int> ParallelRadixSortWithOddEvenMerge(std::vector<int> root_arr);
std::vector<int> EvenCountProcess(std::vector<int> local_arr, int partner);
void OddCountProcess(std::vector<int> local_arr, int partner);

std::vector<int> GetRandomData(int amount, int dist);

#endif  // MODULES_TASK_3_OLYNIN_A_RADIX_SORT_ODD_EVEN_MERGE_RADIX_SORT_ODD_EVEN_MERGE_H_
\end{lstlisting}


\begin{lstlisting}
// radix_sort_odd_even_merge.cpp
// Copyright 2021 Olynin Alexandr
#include <mpi.h>
#include <random>
#include <ctime>
#include "../../../modules/task_3/olynin_a_radix_sort_odd_even_merge/radix_sort_odd_even_merge.h"

std::vector<int> RadixSort(std::vector<int> main_data, int n) {
    std::vector<int> sorted_data[10];
    int koef = 0;
    int max_power = GetMaxPower(main_data);
    while (koef < max_power) {
        for (int i = 0; i < n; i++) {
            int digit = main_data[i] / pow(10, koef);
            digit = digit % 10;
            sorted_data[digit].push_back(main_data[i]);
        }

        main_data.clear();
        for (int i = 0; i < 10; i++) {
            for (size_t j = 0; j < sorted_data[i].size(); j++) {
                main_data.push_back(sorted_data[i][j]);
            }
            sorted_data[i].clear();
        }
        koef++;
    }

    return main_data;
}

int GetMaxPower(std::vector<int> data) {
    int max_power = 0;
    int size = data.size();
    while (size > 0) {
        max_power++;
        for (size_t i = 0; i < data.size(); i++) {
            int div = data[i] / powf(10, max_power);
            if (div == 0)
                size--;
        }
    }

    return max_power;
}

std::vector<int> Merge(std::vector<int> first, std::vector<int> second) {
    int nf = first.size();
    int ns = second.size();
    std::vector<int> result(nf + ns);
    if (nf == 0 && ns == 0)
        return result;

    if (nf < ns)
        return Merge(second, first);

    first.push_back(abs(first[nf - 1] + 1));
    if (ns > 0)
        first[nf] += abs(second[ns - 1]);

    second.push_back(first[nf]);
    int count = 0;
    int i = 0;
    int j = 0;
    while (count < nf + ns) {
        if (first[i] < second[j]) {
            result[count] = first[i++];
            count++;
        } else {
            result[count] = second[j++];
            count++;
        }
    }

    return result;
}

std::vector<int> OddEvenMerge(std::vector<int> first, std::vector<int> second) {
    std::vector<int> first_odd_tmp = first;
    std::vector<int> second_odd_tmp = second;
    std::vector<int> first_even_tmp = first;
    std::vector<int> second_even_tmp = second;

    EvenSplitter(&first_even_tmp, &second_even_tmp);
    OddSplitter(&first_odd_tmp, &second_odd_tmp);

    first = Merge(first_even_tmp, second_even_tmp);
    second = Merge(first_odd_tmp, second_odd_tmp);

    return GetResult(first, second);
}

void EvenSplitter(std::vector<int>* first, std::vector<int>* second) {
    std::vector<int> even_tmp_buf_1;
    std::vector<int> even_tmp_buf_2;
    for (size_t i = 0; i < first->size(); i += 2)
        even_tmp_buf_1.push_back((*first)[i]);

    for (size_t i = 0; i < second->size(); i += 2)
        even_tmp_buf_2.push_back((*second)[i]);

    *first = even_tmp_buf_1;
    *second = even_tmp_buf_2;
}

void OddSplitter(std::vector<int>* first, std::vector<int>* second) {
    std::vector<int> odd_tmp_buf_1;
    std::vector<int> odd_tmp_buf_2;
    for (size_t i = 1; i < first->size(); i += 2)
        odd_tmp_buf_1.push_back((*first)[i]);

    for (size_t i = 1; i < second->size(); i += 2)
        odd_tmp_buf_2.push_back((*second)[i]);

    *first = odd_tmp_buf_1;
    *second = odd_tmp_buf_2;
}

std::vector<int> GetResult(std::vector<int> first, std::vector<int> second) {
    std::vector<int> res;
    for (size_t i = 0; i < first.size() || i < second.size(); i++) {
        if (i < first.size())
            res.push_back(first[i]);

        if (i < second.size())
            res.push_back(second[i]);
    }

    for (size_t i = 1; i < first.size() + second.size() - 1; i += 2) {
        if (res[i] > res[i + 1])
            std::swap(res[i], res[i + 1]);
    }

    return res;
}

std::vector<int> ParallelRadixSortWithOddEvenMerge(std::vector<int> data_root) {
    int ProcRank, ProcNum;
    MPI_Comm_size(MPI_COMM_WORLD, &ProcNum);
    MPI_Comm_rank(MPI_COMM_WORLD, &ProcRank);

    std::vector<int> send_counts(ProcNum);
    std::vector<int> offset(ProcNum);

    if (ProcRank == 0) {
        int sum = 0;
        size_t remains = data_root.size() % ProcNum;
        for (int i = 0; i < ProcNum; i++) {
            send_counts[i] = data_root.size() / ProcNum;
            if (remains > 0) {
                send_counts[i]++;
                remains--;
            }
            offset[i] = sum;
            sum += send_counts[i];
        }
    }

    MPI_Bcast(send_counts.data(), ProcNum, MPI_INT, 0, MPI_COMM_WORLD);
    MPI_Bcast(offset.data(), ProcNum, MPI_INT, 0, MPI_COMM_WORLD);
    std::vector<int> data_local(send_counts[ProcRank]);
    MPI_Scatterv(data_root.data(), send_counts.data(), offset.data(),
                MPI_INT, data_local.data(), send_counts[ProcRank], MPI_INT, 0, MPI_COMM_WORLD);
    data_local = RadixSort(data_local, data_local.size());

    int shift_rank = ProcRank;
    int shift_num = ProcNum;
    bool flag = true;
    int count = 1;
    int odd = 0;
    MPI_Barrier(MPI_COMM_WORLD);
    while (floor(shift_num / 2)) {
        if (shift_num % 2 == 1 && flag) {
            if (ProcRank == ProcNum - 2 * count - odd)
                data_local = EvenCountProcess(data_local, ProcNum - count - odd);

            if (ProcRank == ProcNum - count - odd) {
                OddCountProcess(data_local, ProcNum - 2 * count - odd);
                flag = false;
            }
        }
        MPI_Barrier(MPI_COMM_WORLD);
        if (shift_rank % 2 == 0 && flag)
            data_local = EvenCountProcess(data_local, ProcRank + count);

        if (shift_rank % 2 == 1 && flag) {
            OddCountProcess(data_local, ProcRank - count);
            flag = false;
        }
        shift_rank = shift_rank / 2;
        if (shift_num % 2 == 1)
            odd += count;

        shift_num = floor(shift_num / 2);
        count *= 2;
        MPI_Barrier(MPI_COMM_WORLD);
    }
    if (flag == false) {
        data_local.clear();
        return data_local;
    }

    return data_local;
}

std::vector<int> EvenCountProcess(std::vector<int> data_local, int partner) {
    std::vector<int> first = data_local;
    int second_size;
    MPI_Recv(&second_size, 1, MPI_INT, partner, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
    std::vector<int> second(second_size);
    MPI_Recv(second.data(), second_size, MPI_INT, partner, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);

    int first_size = first.size();
    MPI_Send(&first_size, 1, MPI_INT, partner, 0, MPI_COMM_WORLD);
    MPI_Send(first.data(), first.size(), MPI_INT, partner, 1, MPI_COMM_WORLD);

    EvenSplitter(&first, &second);

    first = Merge(first, second);

    MPI_Recv(&second_size, 1, MPI_INT, partner, 2, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
    second.resize(second_size);
    MPI_Recv(second.data(), second_size, MPI_INT, partner, 3, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
    data_local.resize(first.size() + second.size());

    return GetResult(first, second);
}

void OddCountProcess(std::vector<int> second, int partner) {
    int second_size = second.size();

    MPI_Send(&second_size, 1, MPI_INT, partner, 0, MPI_COMM_WORLD);
    MPI_Send(second.data(), second.size(), MPI_INT, partner, 1, MPI_COMM_WORLD);

    int first_size;
    MPI_Recv(&first_size, 1, MPI_INT, partner, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
    std::vector<int> first(first_size);
    MPI_Recv(first.data(), first_size, MPI_INT, partner, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);

    OddSplitter(&first, &second);

    second = Merge(first, second);
    second_size = second.size();
    MPI_Send(&second_size, 1, MPI_INT, partner, 2, MPI_COMM_WORLD);
    MPI_Send(second.data(), second.size(), MPI_INT, partner, 3, MPI_COMM_WORLD);
}

std::vector<int> GetRandomData(int amount, int dist) {
    std::vector<int> rand_data(amount);
    std::mt19937 gen(time(0));
    for (int i = 0; i < amount; i++)
        rand_data[i] = gen() % dist;

    return rand_data;
}

\end{lstlisting}

\begin{lstlisting}
// main.cpp
// Copyright 2021 Olynin Alexandr
#include <gtest/gtest.h>
#include <gtest-mpi-listener.hpp>
#include "./radix_sort_odd_even_merge.h"

TEST(Accessory_Functions_Tests, RadixSort_test) {
    int ProcRank;
    MPI_Comm_rank(MPI_COMM_WORLD, &ProcRank);

    if (ProcRank == 0) {
        std::vector<int> data;
        int amount = 100;
        data = GetRandomData(amount, 1000);
        std::vector<int> actual_data = RadixSort(data, data.size());
        sort(data.begin(), data.end());
        ASSERT_TRUE(data == actual_data);
    }
    MPI_Barrier(MPI_COMM_WORLD);
}

TEST(Accessory_Functions_Tests, Merge_test) {
    int ProcRank;
    MPI_Comm_rank(MPI_COMM_WORLD, &ProcRank);

    if (ProcRank == 0) {
        std::vector<int> data_first = {2, 8, 9, 20, 42, 129};
        std::vector<int> data_second = {0, 10, 42, 43, 88, 90, 120};
        std::vector<int> expected_data = {0, 2, 8, 9, 10, 20, 42, 42, 43, 88, 90, 120, 129};
        std::vector<int> actual_data = Merge(data_first, data_second);
        ASSERT_TRUE(expected_data == actual_data);
    }
    MPI_Barrier(MPI_COMM_WORLD);
}

TEST(Accessory_Functions_Tests, Sequential_odd_even_merge_test) {
    int ProcRank;
    MPI_Comm_rank(MPI_COMM_WORLD, &ProcRank);

    if (ProcRank == 0) {
        std::vector<int> data_first = {2, 8, 9, 20, 42, 129};
        std::vector<int> data_second = {0, 10, 42, 43, 88, 90, 120};
        std::vector<int> expected_data = {0, 2, 8, 9, 10, 20, 42, 42, 43, 88, 90, 120, 129};
        std::vector<int> actual_data = OddEvenMerge(data_first, data_second);
        ASSERT_TRUE(expected_data == actual_data);
    }
    MPI_Barrier(MPI_COMM_WORLD);
}

TEST(Parallel_Radix_Sort_Tests, Parallel_radix_sort_small_test) {
    int ProcNum, ProcRank;
    MPI_Comm_size(MPI_COMM_WORLD, &ProcNum);
    MPI_Comm_rank(MPI_COMM_WORLD, &ProcRank);

    std::vector<int> data;
    if (ProcRank == 0) {
        int amount = 1000;
        if (ProcNum <= amount)
            data = GetRandomData(amount, 100);
        else
            data = GetRandomData(ProcNum, 100);
    }

    std::vector<int> actual_data = ParallelRadixSortWithOddEvenMerge(data);

    if (ProcRank == 0) {
        std::vector<int> expected_data = RadixSort(data, data.size());
        ASSERT_TRUE(expected_data == actual_data);
    }
    MPI_Barrier(MPI_COMM_WORLD);
}

TEST(Parallel_Radix_Sort_Tests, Parallel_radix_sort_large_test) {
    int ProcNum, ProcRank;
    MPI_Comm_size(MPI_COMM_WORLD, &ProcNum);
    MPI_Comm_rank(MPI_COMM_WORLD, &ProcRank);

    std::vector<int> data;
    if (ProcRank == 0) {
        int amount = 100;
        if (ProcNum <= amount)
            data = GetRandomData(amount, 1000);
        else
            data = GetRandomData(ProcNum, 1000);
    }

    std::vector<int> actual_data = ParallelRadixSortWithOddEvenMerge(data);

    if (ProcRank == 0) {
        std::vector<int> expected_data = RadixSort(data, data.size());
        ASSERT_TRUE(expected_data == actual_data);
    }
    MPI_Barrier(MPI_COMM_WORLD);
}

int main(int argc, char** argv) {
    ::testing::InitGoogleTest(&argc, argv);
    MPI_Init(&argc, &argv);

    ::testing::AddGlobalTestEnvironment(new GTestMPIListener::MPIEnvironment);
    ::testing::TestEventListeners& listeners =
        ::testing::UnitTest::GetInstance()->listeners();

    listeners.Release(listeners.default_result_printer());
    listeners.Release(listeners.default_xml_generator());

    listeners.Append(new GTestMPIListener::MPIMinimalistPrinter);
    return RUN_ALL_TESTS();
}
\end{lstlisting}

\end{document}
